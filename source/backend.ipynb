{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-08-25T16:02:12.558440Z",
     "iopub.status.busy": "2025-08-25T16:02:12.558195Z",
     "iopub.status.idle": "2025-08-25T16:04:25.796332Z",
     "shell.execute_reply": "2025-08-25T16:04:25.795410Z",
     "shell.execute_reply.started": "2025-08-25T16:02:12.558410Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chromadb\n",
      "  Downloading chromadb-1.0.20-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: google-genai in /usr/local/lib/python3.11/dist-packages (1.21.1)\n",
      "Collecting google-genai\n",
      "  Downloading google_genai-1.31.0-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.1/43.1 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting neo4j\n",
      "  Downloading neo4j-5.28.2-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting bert-score\n",
      "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting RapidFuzz\n",
      "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.11.7)\n",
      "Collecting pybase64>=1.4.1 (from chromadb)\n",
      "  Downloading pybase64-1.4.2-cp311-cp311-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.3)\n",
      "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.26.4)\n",
      "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
      "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.14.0)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Downloading onnxruntime-1.22.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_api-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_sdk-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.2)\n",
      "Collecting pypika>=0.48.9 (from chromadb)\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.73.1)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\n",
      "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.16.0)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\n",
      "  Downloading kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (8.5.0)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.0.2)\n",
      "Collecting mmh3>=4.0.1 (from chromadb)\n",
      "  Downloading mmh3-5.2.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.18)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (14.0.0)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.24.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.52.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.1)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (4.9.0)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.40.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.32.4)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (15.0.1)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from neo4j) (2025.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.2.3)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert-score) (3.7.2)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (1.3.1)\n",
      "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (4.9.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.5.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.25.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.5.0)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb) (2.4.1)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_proto-1.36.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading protobuf-6.32.0-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Collecting opentelemetry-semantic-conventions==0.57b0 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n",
      "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.28.1->google-genai) (3.4.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.4.8)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (3.0.9)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-genai) (0.6.1)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.22.5->chromadb) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.22.5->chromadb) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.22.5->chromadb) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.22.5->chromadb) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.22.5->chromadb) (2024.2.0)\n",
      "Downloading chromadb-1.0.20-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentence_transformers-5.1.0-py3-none-any.whl (483 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m483.4/483.4 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_genai-1.31.0-py3-none-any.whl (231 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.9/231.9 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading neo4j-5.28.2-py3-none-any.whl (313 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.2/313.2 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl (61.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.10.1-py3-none-any.whl (374 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.9/374.9 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mmh3-5.2.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading onnxruntime-1.22.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m94.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_api-1.36.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.36.0-py3-none-any.whl (72 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_sdk-1.36.0-py3-none-any.whl (119 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl (201 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pybase64-1.4.2-cp311-cp311-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
      "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-6.32.0-cp39-abi3-manylinux2014_x86_64.whl (322 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.0/322.0 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (453 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pypika\n",
      "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=388f1ec74533437e7b60aa9224a6459c28315c57761daef6efbec877801ac8ab\n",
      "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
      "Successfully built pypika\n",
      "Installing collected packages: pypika, durationpy, uvloop, RapidFuzz, python-dotenv, pybase64, protobuf, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, neo4j, mmh3, humanfriendly, httptools, bcrypt, backoff, watchfiles, posthog, opentelemetry-proto, opentelemetry-api, nvidia-cusparse-cu12, nvidia-cudnn-cu12, coloredlogs, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, nvidia-cusolver-cu12, kubernetes, google-genai, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-grpc, onnxruntime, sentence-transformers, chromadb, bitsandbytes, bert-score, accelerate\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "  Attempting uninstall: google-genai\n",
      "    Found existing installation: google-genai 1.21.1\n",
      "    Uninstalling google-genai-1.21.1:\n",
      "      Successfully uninstalled google-genai-1.21.1\n",
      "  Attempting uninstall: sentence-transformers\n",
      "    Found existing installation: sentence-transformers 4.1.0\n",
      "    Uninstalling sentence-transformers-4.1.0:\n",
      "      Successfully uninstalled sentence-transformers-4.1.0\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 1.8.1\n",
      "    Uninstalling accelerate-1.8.1:\n",
      "      Successfully uninstalled accelerate-1.8.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "google-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 6.32.0 which is incompatible.\n",
      "google-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 6.32.0 which is incompatible.\n",
      "pandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n",
      "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.32.0 which is incompatible.\n",
      "google-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\n",
      "dataproc-spark-connect 0.7.5 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\n",
      "tensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.32.0 which is incompatible.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "bigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed RapidFuzz-3.13.0 accelerate-1.10.1 backoff-2.2.1 bcrypt-4.3.0 bert-score-0.3.13 bitsandbytes-0.47.0 chromadb-1.0.20 coloredlogs-15.0.1 durationpy-0.10 google-genai-1.31.0 httptools-0.6.4 humanfriendly-10.0 kubernetes-33.1.0 mmh3-5.2.0 neo4j-5.28.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 onnxruntime-1.22.1 opentelemetry-api-1.36.0 opentelemetry-exporter-otlp-proto-common-1.36.0 opentelemetry-exporter-otlp-proto-grpc-1.36.0 opentelemetry-proto-1.36.0 opentelemetry-sdk-1.36.0 opentelemetry-semantic-conventions-0.57b0 posthog-5.4.0 protobuf-6.32.0 pybase64-1.4.2 pypika-0.48.9 python-dotenv-1.1.1 sentence-transformers-5.1.0 uvloop-0.21.0 watchfiles-1.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    " pip install chromadb sentence-transformers google-genai neo4j -U bitsandbytes accelerate bert-score RapidFuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-08-25T16:04:25.798102Z",
     "iopub.status.busy": "2025-08-25T16:04:25.797511Z",
     "iopub.status.idle": "2025-08-25T16:05:02.983357Z",
     "shell.execute_reply": "2025-08-25T16:05:02.982724Z",
     "shell.execute_reply.started": "2025-08-25T16:04:25.798073Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-25 16:04:45.055501: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756137885.414567      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756137885.514626      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    }
   ],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import json, torch, gc, chromadb, os, pickle\n",
    "from collections import defaultdict\n",
    "from bert_score import score as bert_score\n",
    "from bert_score import BERTScorer\n",
    "from sentence_transformers.util import cos_sim\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from chromadb.utils import embedding_functions\n",
    "from tqdm import tqdm\n",
    "from google import genai\n",
    "from neo4j import GraphDatabase\n",
    "from google.genai import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T16:05:02.985998Z",
     "iopub.status.busy": "2025-08-25T16:05:02.985215Z",
     "iopub.status.idle": "2025-08-25T16:08:01.509487Z",
     "shell.execute_reply": "2025-08-25T16:08:01.508649Z",
     "shell.execute_reply.started": "2025-08-25T16:05:02.985975Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19eee025695d47a3843d2f6f75b94178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9859fb4d4c04386a0573276857a6146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ca05f87572e42d5a04f82a12600a7b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b51bfbc971e44541b9570f23a4c8ce19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96d6de4385c240ada82092f2f0f675ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/605 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a28fd28a878446a4a11f7a6290058ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/613 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7deae43b53644880bc602447bb408264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/709 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c40c0ca39464f0ca19b1e22ce037abc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b0f15558c404248a4923cfc3bf7bf52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce830b95a4e45c892e328986416fc76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3c9c49404d5443aa1aada373a5fbdec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d96b9bfd67e84f14a64ccc8493ffd465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f312f4bcf66430faaa1dce350315256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f08b0194192041bb8eb2b9f6ab7cce2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04a148409d8b4b61b7331b32205297fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch, re\n",
    "\n",
    "# Load model + tokenizer\n",
    "llm_path = 'AITeamVN/Vi-Qwen2-7B-RAG'\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_path, trust_remote_code=True)\n",
    "\n",
    "max_memory = {\n",
    "    0: \"14GiB\",  # GPU 0\n",
    "    1: \"14GiB\",  # GPU 1\n",
    "    \"cpu\": \"30GiB\" \n",
    "}\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # hoặc float16 nếu gặp lỗi\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"AITeamVN/Vi-Qwen2-7B-RAG\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"AITeamVN/Vi-Qwen2-7B-RAG\", use_fast=False)\n",
    "\n",
    "# model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T16:08:01.511355Z",
     "iopub.status.busy": "2025-08-25T16:08:01.511054Z",
     "iopub.status.idle": "2025-08-25T16:08:05.481197Z",
     "shell.execute_reply": "2025-08-25T16:08:05.480413Z",
     "shell.execute_reply.started": "2025-08-25T16:08:01.511329Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Precedent DB.\n",
      "Connected to Law DB.\n"
     ]
    }
   ],
   "source": [
    "# Precedent database\n",
    "PRECEDENT_DB_URI = \"neo4j+s://06cb763b.databases.neo4j.io\"\n",
    "PRECEDENT_DB_AUTH = (\"neo4j\", \"h9ApWqGVBKuEv3JFuuw7D1VHDPm_DdwE-GQ__7nkwqY\")\n",
    "\n",
    "precedent_driver = GraphDatabase.driver(PRECEDENT_DB_URI, auth=PRECEDENT_DB_AUTH)\n",
    "precedent_driver.verify_connectivity()\n",
    "print(\"Connected to Precedent DB.\")\n",
    "\n",
    "# Law database\n",
    "LAW_DB_URI = \"neo4j+s://504fffc1.databases.neo4j.io\"\n",
    "LAW_DB_AUTH = (\"neo4j\", \"yqAInILZisA5ZYCxJX6s8dSfmae7Th22KxLGysRN65M\")\n",
    "\n",
    "law_driver = GraphDatabase.driver(LAW_DB_URI, auth=LAW_DB_AUTH)\n",
    "law_driver.verify_connectivity()\n",
    "print(\"Connected to Law DB.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Án lệ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T16:08:05.482392Z",
     "iopub.status.busy": "2025-08-25T16:08:05.482107Z",
     "iopub.status.idle": "2025-08-25T16:08:18.683085Z",
     "shell.execute_reply": "2025-08-25T16:08:18.682361Z",
     "shell.execute_reply.started": "2025-08-25T16:08:05.482361Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08a12a267c4d4fe09544f69ed40a2fd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c0d58cb557d49d284ea866a0abc10e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/195 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49cb4a9f8f4e4b6fa8fc0248c654acaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4413a04b58104675a028866982e8444b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0751b5e334234de4a581b5c56db06b80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/707 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d1cf04b32064360a298de06844f480f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c98b209173e4110b23e47345d40c40e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e3c186631d4550b802be90f0320e61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "015b8336a3ca4a06b4b657b9a7ff68f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45dbaa40dc2a44e6b6d9b8fb4edca72e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "1it [00:00,  7.98it/s]\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "precedentEmbedModel = SentenceTransformer(\"truro7/vn-law-embedding\", device='cuda')\n",
    "client = chromadb.Client()\n",
    "collection_name = \"vn_precedent_docs\"\n",
    "\n",
    "existing_collections = [col.name for col in client.list_collections()]\n",
    "if collection_name in existing_collections:\n",
    "    client.delete_collection(name=collection_name)\n",
    "\n",
    "# 5. Tạo mới collection\n",
    "collection = client.create_collection(name=collection_name)\n",
    "\n",
    "file_path = \"/kaggle/input/precedent-embedding/precedent_embeddings.jsonl\"\n",
    "texts, embeddings, metadata_list = [], [], []\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        texts.append(data[\"text\"])\n",
    "        embeddings.append(data[\"embedding\"])\n",
    "        metadata_list.append(data[\"metadata\"])\n",
    "\n",
    "# Add vào Chroma theo batch\n",
    "def batch_iter(data, meta, emb, batch_size):\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        yield data[i:i+batch_size], meta[i:i+batch_size], emb[i:i+batch_size], i\n",
    "\n",
    "batch_size = 2000\n",
    "for batch_texts, batch_meta, batch_emb, start_idx in tqdm(batch_iter(texts, metadata_list, embeddings, batch_size)):\n",
    "    batch_ids = [f\"id_{i}\" for i in range(start_idx, start_idx + len(batch_texts))]\n",
    "    collection.add(\n",
    "        documents=batch_texts,\n",
    "        embeddings=batch_emb,\n",
    "        ids=batch_ids,\n",
    "        metadatas=batch_meta\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T16:13:28.369579Z",
     "iopub.status.busy": "2025-08-25T16:13:28.369289Z",
     "iopub.status.idle": "2025-08-25T16:14:50.148763Z",
     "shell.execute_reply": "2025-08-25T16:14:50.148143Z",
     "shell.execute_reply.started": "2025-08-25T16:13:28.369559Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "439801455e15429f8fe90ebacc5ea254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07494dc9e38e49999d51aa6750d00b28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/171 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f145ca7c691949fca92df1728de9349c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "190d364db8bd4c0e82a73453b5d4057e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/54.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daf9e47998bf4c48b16f9a20674bd369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/664 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a7f51cc1b6c4edc9651eb8a23e0c71c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.27G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b92e10d2cc7d45348d997dfdd780ade9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1caf283cbf9b4407bb3fd162d501ced9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11ef5cab337643dc97e994e7517482a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81ad542dc8224a5a9a57c1a57263a0e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fe42e5d6b3144f781d559e836d006dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/297 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:46,  2.90s/it]\n"
     ]
    }
   ],
   "source": [
    "lawEmbedModel = SentenceTransformer(\"AITeamVN/Vietnamese_Embedding_v2\",device='cuda')\n",
    "client = chromadb.Client()\n",
    "collection_name = \"vn_law_docs\"\n",
    "\n",
    "existing_collections = [col.name for col in client.list_collections()]\n",
    "if collection_name in existing_collections:\n",
    "    client.delete_collection(name=collection_name)\n",
    "\n",
    "# 5. Tạo mới collection\n",
    "collection = client.create_collection(name=collection_name)\n",
    "\n",
    "file_path = \"/kaggle/input/law-embedding/law_embeddings.jsonl\"\n",
    "texts, embeddings, metadata_list = [], [], []\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        texts.append(data[\"text\"])\n",
    "        embeddings.append(data[\"embedding\"])\n",
    "        metadata_list.append(data[\"metadata\"])\n",
    "        \n",
    "# Thêm vào Chroma theo batch\n",
    "def batch_iter(data, meta, emb, batch_size):\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        yield data[i:i+batch_size], meta[i:i+batch_size], emb[i:i+batch_size], i\n",
    "\n",
    "batch_size = 2000\n",
    "for batch_texts, batch_meta, batch_emb, start_idx in tqdm(batch_iter(texts, metadata_list, embeddings, batch_size)):\n",
    "    batch_ids = [f\"law_id_{i}\" for i in range(start_idx, start_idx + len(batch_texts))]\n",
    "    collection.add(\n",
    "        documents=batch_texts,       \n",
    "        embeddings=batch_emb,\n",
    "        ids=batch_ids,\n",
    "        metadatas=batch_meta\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T16:16:03.327163Z",
     "iopub.status.busy": "2025-08-25T16:16:03.326630Z",
     "iopub.status.idle": "2025-08-25T16:16:50.530428Z",
     "shell.execute_reply": "2025-08-25T16:16:50.529836Z",
     "shell.execute_reply.started": "2025-08-25T16:16:03.327123Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def suit_precedent(result):\n",
    "    client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "    \n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        config=types.GenerateContentConfig(\n",
    "            system_instruction=\"Hãy đánh giá và so sánh tình huống pháp lý trong yêu cầu của người dùng với các án lệ được truy xuất. Tập trung vào mức độ tương đồng về tình tiết, sự kiện pháp lý và vấn đề pháp lý được giải quyết trong án lệ. Chỉ trả về 1 mã án lệ phù hợp nhất (ví dụ: 01/2016/AL, 02/2016/AL, 63/2022/AL) nếu tình huống người dùng và án lệ có sự tương đồng rõ ràng về mặt pháp lý. Nếu không có án lệ nào đủ tương đồng, trả về một xâu rỗng. Không giải thích, không lập luận, không kèm thêm nội dung nào khác ngoài mã án lệ hoặc xâu rỗng.\"),\n",
    "        contents=result\n",
    "    )\n",
    "    \n",
    "    data = response.text.strip() if response.text else \"\"\n",
    "\n",
    "    return data if data else None\n",
    "    \n",
    "def insert_summary(tx, case_number, summary_text):\n",
    "    tx.run(\"\"\"\n",
    "        MERGE (c:Case {case_number: $case_number})\n",
    "        CREATE (s:Summary {text: $summary})\n",
    "        MERGE (c)-[:HAS_SUMMARY]->(s)\n",
    "    \"\"\", case_number=case_number, summary=summary_text)\n",
    "\n",
    "with precedent_driver.session() as session:\n",
    "    with open(\"/kaggle/input/summary-an-le/summary_an_le.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            if data.get(\"section\") == \"Nội dung vụ án\":\n",
    "                session.execute_write(insert_summary, data[\"case_number\"], data[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T16:19:54.125308Z",
     "iopub.status.busy": "2025-08-25T16:19:54.125046Z",
     "iopub.status.idle": "2025-08-25T16:19:54.134570Z",
     "shell.execute_reply": "2025-08-25T16:19:54.133836Z",
     "shell.execute_reply.started": "2025-08-25T16:19:54.125291Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def build_context(precedent_data: list, applied_laws: dict) -> str:\n",
    "    context_parts = []\n",
    "    for precedent in precedent_data:\n",
    "        case_number = precedent.get(\"case_number\", \"Không rõ mã án lệ\")\n",
    "        summary = precedent.get(\"summary\", \"\")\n",
    "        decision = precedent.get(\"decision\", \"\")\n",
    "        core = precedent.get(\"core\", \"\")\n",
    "        \n",
    "        text = f\"Án lệ: {case_number}\\n\" \\\n",
    "               f\"- Tóm tắt: {summary}\\n\" \\\n",
    "               f\"- Quyết định: {decision}\\n\" \\\n",
    "               f\"- Nội dung án lệ: {core}\\n\"\n",
    "\n",
    "        # Ghép điều luật tương ứng nếu có\n",
    "        laws = applied_laws.get(case_number, [])\n",
    "        if laws:\n",
    "            text += \"- Các luật được dùng:\\n\"\n",
    "            for law in laws:\n",
    "                law_text = f\"  + Điều luật: {law['title']} ({law['law']})\\n\" \\\n",
    "                           f\"    Nội dung: {law['content']}\"\n",
    "                text += law_text + \"\\n\"\n",
    "\n",
    "        context_parts.append(text.strip())\n",
    "\n",
    "    return \"\\n\\n\".join(context_parts)\n",
    "\n",
    "def get_precedent_answer(query: str, precedent_data: list, applied_laws: list, max_new_tokens: int = 2048) -> str:\n",
    "    context = build_context(precedent_data, applied_laws)\n",
    "    \n",
    "    # Prompt hướng dẫn\n",
    "    SYSTEM_PROMPT = '''Bạn là một trợ lý pháp lý. Khi người dùng cung cấp một tình huống pháp lý và đặt câu hỏi, hoặc kèm theo thông tin tham khảo như điều luật, án lệ, văn bản pháp luật, hãy trả lời theo các nguyên tắc sau:\n",
    "    1. Ngôn ngữ: Đảm bảo trả lời hoàn toàn bằng *tiếng Việt, đúng chính tả, không sử dụng từ ngữ nước ngoài.*\n",
    "    2.Nội dung:\n",
    "    Trả lời ngắn gọn, chính xác, bám sát câu hỏi.\n",
    "    Không tự suy diễn hoặc thêm tình tiết không có trong tình huống.\n",
    "    Không sử dụng lời chào, văn phong cảm tính hay hành chính.\n",
    "    3. Sử dụng căn cứ pháp lý:\n",
    "    Nếu áp dụng điều luật, phải nêu rõ số điều và trích nguyên văn nội dung áp dụng.\n",
    "    Nếu sử dụng án lệ, nêu rõ số án lệ, nội dung cốt lõi và lý do liên quan đến tình huống.\n",
    "    4. Nếu thiếu thông tin hoặc không có căn cứ pháp lý rõ ràng:\n",
    "    Trả lời: “Không đủ thông tin để kết luận” hoặc “Không có căn cứ pháp lý rõ ràng để áp dụng trong trường hợp này.”\n",
    "    5. Không đưa lời khuyên cảm tính hoặc đạo đức.\n",
    "    Không sử dụng các cụm như “nên làm”, “cần phải” nếu không có căn cứ pháp lý cụ thể.'''\n",
    "    \n",
    "    TEMPLATE = \"\"\"\n",
    "    Tình huống pháp lý:\n",
    "    {question}\n",
    "    \n",
    "    Các căn cứ pháp lý có thể áp dụng (án lệ, điều luật...):\n",
    "    {context}\n",
    "    \n",
    "    **Hãy cung cấp trả lời bám sát và phù hợp với câu hỏi của người dùng**\n",
    "    \"\"\"\n",
    "\n",
    "    conversation = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": TEMPLATE.format(context=context, question=query)}\n",
    "    ]\n",
    "\n",
    "    text = tokenizer.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    " \n",
    "    generated_ids = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=0.3,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.3,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    print(\"Token length:\", len(inputs['input_ids'][0]))\n",
    "    generated_ids = [output[len(input_ids):] for input_ids, output in zip(inputs.input_ids, generated_ids)]\n",
    "    decoded = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    cleaned = re.split(r\"(### Câu hỏi:)\", decoded)[0]\n",
    "    return cleaned.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T16:19:56.870546Z",
     "iopub.status.busy": "2025-08-25T16:19:56.870280Z",
     "iopub.status.idle": "2025-08-25T16:19:56.876638Z",
     "shell.execute_reply": "2025-08-25T16:19:56.876029Z",
     "shell.execute_reply.started": "2025-08-25T16:19:56.870527Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_precedent(precedent_list):\n",
    "    result = []\n",
    "    for precedent_number in precedent_list:\n",
    "        records, summary, keys = precedent_driver.execute_query(\n",
    "            \"\"\"\n",
    "            MATCH (c:Case {case_number: $precedent_number})\n",
    "            OPTIONAL MATCH (c)-[:HAS_SUMMARY]->(sm:Summary)\n",
    "            OPTIONAL MATCH (c)-[:HAS_DECISION]->(dc:Decision)\n",
    "            OPTIONAL MATCH (c)-[:HAS_PRECEDENT_CONTENT]->(pc:PrecedentContent)\n",
    "            RETURN \n",
    "              c.case_number AS case_number,\n",
    "              head(collect(sm.text)) AS summary,\n",
    "              head(collect(dc.text)) AS decision,\n",
    "              head(collect(pc.text)) AS core\n",
    "            \"\"\",\n",
    "            precedent_number=precedent_number\n",
    "        )\n",
    "        for record in records:\n",
    "            result.append({\"case_number\": record[\"case_number\"],\"summary\": record.get(\"summary\", \"\"),\n",
    "                           \"decision\": record.get(\"decision\", \"\"),\"core\": record.get(\"core\", \"\")})\n",
    "    return result\n",
    "\n",
    "def get_applied_law(precedent_list):\n",
    "    result = {}\n",
    "    for precedent_number in precedent_list:\n",
    "        records, _, _ = law_driver.execute_query(\n",
    "            \"\"\"\n",
    "            MATCH (c:Case {case_number: $precedent_number})-[:HAS_APPLIED]->(n:Article)\n",
    "            MATCH (n)-[:HAS_CHUNK]->(a:Chunk)\n",
    "            WHERE n.code = a.code\n",
    "            WITH n.title AS title, n.code AS law_name, collect(a.content) AS chunks\n",
    "            RETURN law_name, title, apoc.text.join(chunks, \" \") AS full_content\n",
    "            UNION\n",
    "            MATCH (c:Case {case_number: $precedent_number})-[:HAS_APPLIED]->(n:Statute)\n",
    "            MATCH (n)-[:HAS_CHUNK]->(a:Chunk)\n",
    "            WHERE n.code = a.code\n",
    "            WITH n.title AS title, n.code AS law_name, collect(a.content) AS chunks\n",
    "            RETURN law_name, title, apoc.text.join(chunks, \" \") AS full_content\n",
    "            \"\"\",\n",
    "            precedent_number=precedent_number\n",
    "        )\n",
    "        laws = []\n",
    "        for record in records:\n",
    "            laws.append({\n",
    "                \"law\": record[\"law_name\"],\n",
    "                \"title\": record[\"title\"],\n",
    "                \"content\": record[\"full_content\"]\n",
    "            })\n",
    "        result[precedent_number] = laws\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T16:19:59.217731Z",
     "iopub.status.busy": "2025-08-25T16:19:59.217071Z",
     "iopub.status.idle": "2025-08-25T16:19:59.223435Z",
     "shell.execute_reply": "2025-08-25T16:19:59.222682Z",
     "shell.execute_reply.started": "2025-08-25T16:19:59.217708Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def retrieve_precedent(query):\n",
    "    client = chromadb.Client()\n",
    "    collection=client.get_collection(\"vn_precedent_docs\")\n",
    "    query_embedding = precedentEmbedModel.encode([query])[0]\n",
    "    \n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=5\n",
    "    )\n",
    "    \n",
    "    THRESHOLD = 1\n",
    "\n",
    "    top_results = results[\"documents\"][0]\n",
    "    distances = results[\"distances\"][0]\n",
    "    metadatas = results[\"metadatas\"][0]\n",
    "\n",
    "    print(sum(distances)/len(distances))\n",
    "    \n",
    "    filtered = [\n",
    "        {\n",
    "            \"case_number\": metadatas[i][\"case_number\"],\n",
    "            \"document\": top_results[i],\n",
    "            \"distance\": distances[i]\n",
    "        }\n",
    "        for i in range(len(top_results))\n",
    "        if distances[i] <= THRESHOLD\n",
    "    ]\n",
    "    \n",
    "    if not filtered:\n",
    "        return \"None\"\n",
    "    \n",
    "    results_text = \"\\n\\n\".join(\n",
    "        f\"Án lệ {r['case_number']} (distance: {r['distance']:.4f}):\\n{r['document']}\"\n",
    "        for r in filtered\n",
    "    )\n",
    "    \n",
    "    final_prompt = f\"Tình huống:\\n{query}\\n\\nCác án lệ phù hợp:\\n{results_text}\"\n",
    "    return final_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T16:20:01.748597Z",
     "iopub.status.busy": "2025-08-25T16:20:01.747937Z",
     "iopub.status.idle": "2025-08-25T16:20:01.754179Z",
     "shell.execute_reply": "2025-08-25T16:20:01.753484Z",
     "shell.execute_reply.started": "2025-08-25T16:20:01.748574Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from google.genai import types\n",
    "def suit_question(query):\n",
    "    client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "    \n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        config=types.GenerateContentConfig(\n",
    "            system_instruction=\"\"\"\n",
    "            Bạn là chuyên gia pháp lý Việt Nam. \n",
    "            Nhiệm vụ: Phân tích tình huống pháp lý được cung cấp và đưa ra 3–5 câu hỏi ngắn gọn, cần thiết để tra cứu các luật dùng để giải quyết cho tình huống.\n",
    "            Chỉ hỏi về quy định pháp luật chung, không đề cập tên riêng, nghề nghiệp hoặc số liệu cụ thể trong tình huống.\n",
    "            Yêu cầu:\n",
    "            - Chỉ viết câu hỏi, không giải thích.\n",
    "            - Ngắn gọn, tối đa 20 từ mỗi câu.\n",
    "            - Không hỏi về chứng cứ hay thủ tục hành chính, chỉ tập trung vào vấn đề pháp lý cốt lõi.\n",
    "            - Chỉ dùng tiếng Việt đúng chính tả.\n",
    "            Tình huống:\n",
    "            \"\"\"),\n",
    "        contents=query\n",
    "    )\n",
    "\n",
    "    data=response.text\n",
    "    data=[x.strip() for x in data.split('\\n') if x.strip()]\n",
    "    return data\n",
    "\n",
    "def query_law(query):\n",
    "    client = chromadb.Client()\n",
    "    all_law=[]\n",
    "    collection=client.get_collection(\"vn_law_docs\")\n",
    "\n",
    "    for q in query:\n",
    "        query_embedding = lawEmbedModel.encode([q])[0]\n",
    "        results = collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=5\n",
    "        )\n",
    "        all_law.append(results)\n",
    "    return all_law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T16:20:03.801201Z",
     "iopub.status.busy": "2025-08-25T16:20:03.800944Z",
     "iopub.status.idle": "2025-08-25T16:20:03.807236Z",
     "shell.execute_reply": "2025-08-25T16:20:03.806432Z",
     "shell.execute_reply.started": "2025-08-25T16:20:03.801184Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def normalized_law(all_results):\n",
    "    normalized_entries = []\n",
    "    seen = set()\n",
    "\n",
    "    for results in all_results:\n",
    "        distances = results[\"distances\"][0]\n",
    "        metadatas = results[\"metadatas\"][0]\n",
    "\n",
    "        for law, statute, distance in zip(\n",
    "            [m[\"law\"] for m in metadatas],\n",
    "            [m[\"statute\"] for m in metadatas],\n",
    "            distances\n",
    "        ):\n",
    "            key_tuple = (law, statute)\n",
    "            if key_tuple in seen:\n",
    "                continue\n",
    "            seen.add(key_tuple)\n",
    "\n",
    "            if \"Bộ luật\" in law:\n",
    "                normalized_key = f\"{law.replace(' ', '_')}_{statute.replace(' ', '').split('.')[0]}\"\n",
    "                is_code = True\n",
    "            else:\n",
    "                normalized_key = f\"{law} {statute}\"\n",
    "                is_code = False\n",
    "\n",
    "            normalized_entries.append({\n",
    "                \"key\": normalized_key,\n",
    "                \"is_code\": is_code,\n",
    "                \"law\": law.strip(),\n",
    "                \"statute\": statute.strip(),\n",
    "                \"distance\": distance\n",
    "            })\n",
    "    return normalized_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T16:20:06.099447Z",
     "iopub.status.busy": "2025-08-25T16:20:06.098878Z",
     "iopub.status.idle": "2025-08-25T16:20:06.186837Z",
     "shell.execute_reply": "2025-08-25T16:20:06.186288Z",
     "shell.execute_reply.started": "2025-08-25T16:20:06.099424Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from rapidfuzz import fuzz\n",
    "def dedup_by_embedding(results, threshold=0.9):\n",
    "    selected = []\n",
    "    for r in results:\n",
    "        is_dup = False\n",
    "        for s in selected:\n",
    "            sim = fuzz.token_set_ratio(r[\"content\"], s[\"content\"])\n",
    "            if sim >= threshold:\n",
    "                # Nếu luật mới \"tốt hơn\" thì thay thế\n",
    "                if r[\"distance\"] < s[\"distance\"]:\n",
    "                    s.update(r)\n",
    "                is_dup = True\n",
    "                break\n",
    "        if not is_dup:\n",
    "            selected.append(r)\n",
    "    return selected\n",
    "\n",
    "def retrieve_laws(normalized_laws):\n",
    "    results = []\n",
    "\n",
    "    for i, item in enumerate(normalized_laws):\n",
    "        key = item[\"key\"]\n",
    "        is_code = item[\"is_code\"]\n",
    "        law = item[\"law\"]\n",
    "        statute = item[\"statute\"]\n",
    "        distance = item[\"distance\"]\n",
    "\n",
    "        id_pattern = f\"(?i).*{key}.*\"\n",
    "\n",
    "        records, _, _ = driver.execute_query(\n",
    "            \"\"\"\n",
    "            MATCH (a:Article)-[:HAS_CHUNK]->(ch:Chunk)\n",
    "            WHERE ch.id =~ $id_pattern AND a.title=$statute\n",
    "            WITH a, ch\n",
    "            ORDER BY ch.index\n",
    "            RETURN a.code AS LawTitle,a.title AS ArticleTitle, collect(ch.content) AS Chunks\n",
    "            UNION\n",
    "            MATCH (s:Statute)-[:HAS_CHUNK]->(ch:Chunk)\n",
    "            WHERE s.law = $law AND s.title CONTAINS $statute\n",
    "            RETURN s.law AS LawTitle,s.title AS ArticleTitle, collect(ch.content) AS Chunks\n",
    "            \"\"\",\n",
    "            id_pattern=id_pattern,law=law,statute=statute\n",
    "        )\n",
    "\n",
    "        for rec in records:\n",
    "            content = \" \".join(rec[\"Chunks\"]).strip()\n",
    "            results.append({\n",
    "                \"LawTitle\": rec[\"LawTitle\"],\n",
    "                \"ArticleTitle\": rec[\"ArticleTitle\"],\n",
    "                \"content\": content,\n",
    "                \"distance\": distance\n",
    "            })\n",
    "    \n",
    "    deduped = dedup_by_embedding(results, threshold=0.90)\n",
    "    filtered= [r for r in results if r[\"distance\"] <= 0.95]\n",
    "    context_str = \"\\n\".join(\n",
    "        f\"{r['ArticleTitle']}, {r['LawTitle']}: {r['content']}\"\n",
    "        for r in filtered\n",
    "    )\n",
    "    return context_str, filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T16:20:08.481597Z",
     "iopub.status.busy": "2025-08-25T16:20:08.481320Z",
     "iopub.status.idle": "2025-08-25T16:20:08.489045Z",
     "shell.execute_reply": "2025-08-25T16:20:08.488216Z",
     "shell.execute_reply.started": "2025-08-25T16:20:08.481575Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_law_answer(law_list, user_query: str, max_new_tokens=1024):\n",
    "    # Prompt hướng dẫn\n",
    "    SYSTEM_PROMPT = '''Bạn là một trợ lý pháp lý. Khi người dùng cung cấp một tình huống pháp lý và \n",
    "    đặt câu hỏi, hoặc kèm theo thông tin tham khảo như điều luật, án lệ, văn bản pháp luật, \n",
    "    hãy trả lời theo các nguyên tắc sau:\n",
    "    1. Ngôn ngữ: Đảm bảo trả lời hoàn toàn bằng *tiếng Việt, đúng chính tả, không sử dụng từ ngữ \n",
    "    nước ngoài.*\n",
    "    2.Nội dung:\n",
    "    Trả lời ngắn gọn, chính xác, bám sát câu hỏi.\n",
    "    Không tự suy diễn hoặc thêm tình tiết không có trong tình huống.\n",
    "    Không sử dụng lời chào, văn phong cảm tính hay hành chính.\n",
    "    3. Sử dụng căn cứ pháp lý:\n",
    "    Nếu áp dụng điều luật, phải nêu rõ số điều và trích nguyên văn nội dung áp dụng.\n",
    "    Nếu sử dụng án lệ, nêu rõ số án lệ, nội dung cốt lõi và lý do liên quan đến tình huống.\n",
    "    Dùng điều luật mới nhất (ví dụ: dùng bộ luật dân sự 2015 thay vì dùng bộ luật dân sự 2005)\n",
    "    **Trong các điều luật được đưa, phải xem xét các điều luật nào là phù hợp để giải quyết \n",
    "    tình huống**\n",
    "    4. Nếu thiếu thông tin hoặc không có căn cứ pháp lý rõ ràng:\n",
    "    Trả lời: “Không đủ thông tin để kết luận” hoặc “Không có căn cứ pháp lý rõ ràng để áp dụng trong \n",
    "    trường hợp này.”\n",
    "    5. Không đưa lời khuyên cảm tính hoặc đạo đức.\n",
    "    Không sử dụng các cụm như “nên làm”, “cần phải” nếu không có căn cứ pháp lý cụ thể.\n",
    "    '''\n",
    "    \n",
    "    TEMPLATE = \"\"\"\n",
    "    Tình huống pháp lý:\n",
    "    {question}\n",
    "    \n",
    "    Các căn cứ pháp lý có thể áp dụng (án lệ, điều luật...):\n",
    "    {context}\n",
    "    \n",
    "    **Hãy cung cấp trả lời bám sát và phù hợp với câu hỏi của người dùng**\n",
    "    \"\"\"\n",
    "    conversation = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT.strip()},\n",
    "        {\"role\": \"user\", \"content\": TEMPLATE.format(\n",
    "            question=user_query.strip(),\n",
    "            context=law_list\n",
    "        )}\n",
    "    ]\n",
    "    \n",
    "    text = tokenizer.apply_chat_template(\n",
    "        conversation,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    \n",
    "    generated_ids = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=0.3,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.3,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    \n",
    "    generated_ids = [output[len(input_ids):] for input_ids, output in zip(inputs.input_ids, generated_ids)]\n",
    "    decoded = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    cleaned = re.split(r\"(### Câu hỏi:)\", decoded)[0].strip()\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BACKEND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T16:20:11.062478Z",
     "iopub.status.busy": "2025-08-25T16:20:11.062240Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "🎯 Public FastAPI endpoint: NgrokTunnel: \"https://f1cee5182c50.ngrok-free.app\" -> \"http://localhost:8000\"/chat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [36]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "# 1. Cài thư viện\n",
    "!pip install -q fastapi uvicorn nest_asyncio pyngrok streamlit\n",
    "from pyngrok import ngrok\n",
    "import threading\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# 2. Set Ngrok token\n",
    "ngrok.set_auth_token(\"2efGJiQaVuc2T3ZvY7GML1DAcl1_51kuQumr7HJrwwUhcujGf\")\n",
    "\n",
    "# 3. Import các thành phần backend\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "import uvicorn\n",
    "\n",
    "# 4. Tạo app FastAPI\n",
    "app = FastAPI()\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"], \n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# 5. Định nghĩa schema\n",
    "class Message(BaseModel):\n",
    "    message: str\n",
    "\n",
    "# 6. Định nghĩa route\n",
    "@app.get(\"/\")\n",
    "def read_root():\n",
    "    return {\"message\": \"Backend đang chạy!\"}\n",
    "\n",
    "@app.post(\"/chat\")\n",
    "def chat_handler(query: Message):\n",
    "    try:\n",
    "        user_query = query.message  # lấy string từ Pydantic object\n",
    "        print(\"-\" * 100)\n",
    "        print(\"Câu hỏi từ UI:\", user_query)\n",
    "        \n",
    "        top5 = retrieve_precedent(user_query)\n",
    "        precedent_code = suit_precedent(top5)\n",
    "        print(precedent_code)\n",
    "        \n",
    "        if precedent_code:\n",
    "            print(5*\"-\", \"Dùng án lệ\", 5*\"-\")\n",
    "            precedents = get_precedent([precedent_code])\n",
    "            laws = get_applied_law([precedent_code])\n",
    "            answer = get_precedent_answer(\n",
    "                query=user_query,   \n",
    "                precedent_data=precedents,\n",
    "                applied_laws=laws\n",
    "            )\n",
    "            print(answer)\n",
    "            return {\"answer\": answer}\n",
    "        else:\n",
    "            print(5*\"-\", \"Dùng luật\", 5*\"-\")\n",
    "            questions = suit_question(user_query)  \n",
    "            laws = query_law(questions)\n",
    "            normalized = normalized_law(laws)\n",
    "            laws_content = retrieve_laws(normalized)\n",
    "            response = get_law_answer(laws_content, user_query)\n",
    "            print(response)\n",
    "            return {\"answer\": response}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Lỗi backend:\", e)\n",
    "        return {\"answer\": f\"Lỗi hệ thống: {str(e)}\"}\n",
    "\n",
    "\n",
    "# 8. Khởi tạo Ngrok\n",
    "public_url = ngrok.connect(8000)\n",
    "print(f\"🎯 Public FastAPI endpoint: {public_url}/chat\")\n",
    "\n",
    "# Anh Nam muốn mua một căn nhà của ông Hùng với giá 3 tỷ đồng. Hai bên đã thỏa thuận miệng và anh Nam đã đặt cọc cho ông Hùng số tiền 300 triệu đồng, có lập văn bản đặt cọc rõ ràng. Trong văn bản có ghi, sau 30 ngày kể từ ngày đặt cọc, hai bên sẽ tiến hành ký hợp đồng mua bán công chứng. Tuy nhiên, sau 20 ngày, giá đất khu vực đó tăng mạnh. Ông Hùng thông báo không bán nhà cho anh Nam nữa và muốn trả lại 300 triệu tiền cọc. Anh Nam không đồng ý và yêu cầu ông Hùng phải bồi thường. \n",
    "# Hỏi yêu cầu của anh Nam có đúng không và ông Hùng phải chịu trách nhiệm như thế nào?\n",
    "uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8084140,
     "sourceId": 12786751,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8084194,
     "sourceId": 12786829,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8102350,
     "sourceId": 12813614,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8102409,
     "sourceId": 12859414,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8133497,
     "sourceId": 12859125,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
